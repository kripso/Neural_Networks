{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# generate a dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, n_grads, H):\n",
    "        super().__init__()\n",
    "        self.grads = np.random.uniform(-1, 1, size=n_grads)\n",
    "        self.x = torch.arange(0, 1, 0.01)\n",
    "        self.H = H # horizon [steps]\n",
    "        self.interval = H//2 # [steps]\n",
    "\n",
    "        self.len = n_grads\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        1. sample `grads`\n",
    "        2. generate linear data (seq_len*input_size)\n",
    "        Remarks: (batch_size*seq_len*input_size) is formed by DataLoader.\n",
    "        \"\"\"\n",
    "        H = self.H\n",
    "\n",
    "        # 1.\n",
    "        grad = self.grads[idx]\n",
    "\n",
    "        # 2.\n",
    "        y = grad * self.x\n",
    "\n",
    "        subys = torch.tensor([])  # (seq_len*input_size(H))\n",
    "        i = 0\n",
    "        while len(y[i:i+H]) == H:\n",
    "            suby = y[i:i+H].view(1, -1)\n",
    "            subys = torch.cat((subys, suby), 0)\n",
    "            i += self.interval\n",
    "\n",
    "        return subys, grad\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.h_n = None  # hidden state\n",
    "        self.c_n = None  # cell state\n",
    "\n",
    "        # define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, 1)  # predicts `linear-grad`\n",
    "\n",
    "    def init_states(self, batch_size):\n",
    "        \"\"\"initialize the hidden and cell states\"\"\"\n",
    "        self.h_n = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "        self.c_n = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        states = (self.h_n, self.c_n)\n",
    "        out, (self.h_n, self.c_n) = self.lstm(x, states)\n",
    "\n",
    "        # get the last one of the (sequential) output from the LSTM\n",
    "        out = out[:, -1, :]  # (batch, hidden_size)\n",
    "        out = self.linear(out)  # (batch, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = MyDataset(n_grads=30, H=10)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=8)\n",
    "\n",
    "val_dataset = MyDataset(n_grads=10, H=10)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "\n",
    "# Model\n",
    "model = LSTM(input_dim=train_dataset.H,hidden_dim=32,num_layers=2)\n",
    "\n",
    "\n",
    "# Compile\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | train_loss: 0.30529 | val_loss: 0.45667\n",
      "epoch: 2 | train_loss: 0.27443 | val_loss: 0.43076\n",
      "epoch: 3 | train_loss: 0.24448 | val_loss: 0.40052\n",
      "epoch: 4 | train_loss: 0.21029 | val_loss: 0.36279\n",
      "epoch: 5 | train_loss: 0.16894 | val_loss: 0.31438\n",
      "epoch: 6 | train_loss: 0.12096 | val_loss: 0.25518\n",
      "epoch: 7 | train_loss: 0.07610 | val_loss: 0.19261\n",
      "epoch: 8 | train_loss: 0.05071 | val_loss: 0.13443\n",
      "epoch: 9 | train_loss: 0.03407 | val_loss: 0.08010\n",
      "epoch: 10 | train_loss: 0.01652 | val_loss: 0.04083\n",
      "epoch: 11 | train_loss: 0.01258 | val_loss: 0.02166\n",
      "epoch: 12 | train_loss: 0.01581 | val_loss: 0.01585\n",
      "epoch: 13 | train_loss: 0.01441 | val_loss: 0.01448\n",
      "epoch: 14 | train_loss: 0.00984 | val_loss: 0.01569\n",
      "epoch: 15 | train_loss: 0.00724 | val_loss: 0.01772\n",
      "epoch: 16 | train_loss: 0.00629 | val_loss: 0.01748\n",
      "epoch: 17 | train_loss: 0.00549 | val_loss: 0.01408\n",
      "epoch: 18 | train_loss: 0.00457 | val_loss: 0.00916\n",
      "epoch: 19 | train_loss: 0.00379 | val_loss: 0.00543\n",
      "epoch: 20 | train_loss: 0.00341 | val_loss: 0.00391\n",
      "epoch: 21 | train_loss: 0.00324 | val_loss: 0.00340\n",
      "epoch: 22 | train_loss: 0.00294 | val_loss: 0.00318\n",
      "epoch: 23 | train_loss: 0.00262 | val_loss: 0.00329\n",
      "epoch: 24 | train_loss: 0.00239 | val_loss: 0.00328\n",
      "epoch: 25 | train_loss: 0.00214 | val_loss: 0.00287\n",
      "epoch: 26 | train_loss: 0.00188 | val_loss: 0.00236\n",
      "epoch: 27 | train_loss: 0.00167 | val_loss: 0.00204\n",
      "epoch: 28 | train_loss: 0.00149 | val_loss: 0.00188\n",
      "epoch: 29 | train_loss: 0.00132 | val_loss: 0.00181\n",
      "epoch: 30 | train_loss: 0.00117 | val_loss: 0.00172\n",
      "epoch: 31 | train_loss: 0.00103 | val_loss: 0.00153\n",
      "epoch: 32 | train_loss: 0.00090 | val_loss: 0.00127\n",
      "epoch: 33 | train_loss: 0.00079 | val_loss: 0.00105\n",
      "epoch: 34 | train_loss: 0.00069 | val_loss: 0.00090\n",
      "epoch: 35 | train_loss: 0.00060 | val_loss: 0.00079\n",
      "epoch: 36 | train_loss: 0.00052 | val_loss: 0.00069\n",
      "epoch: 37 | train_loss: 0.00046 | val_loss: 0.00058\n",
      "epoch: 38 | train_loss: 0.00039 | val_loss: 0.00048\n",
      "epoch: 39 | train_loss: 0.00034 | val_loss: 0.00040\n",
      "epoch: 40 | train_loss: 0.00030 | val_loss: 0.00035\n",
      "epoch: 41 | train_loss: 0.00026 | val_loss: 0.00030\n",
      "epoch: 42 | train_loss: 0.00022 | val_loss: 0.00026\n",
      "epoch: 43 | train_loss: 0.00019 | val_loss: 0.00022\n",
      "epoch: 44 | train_loss: 0.00017 | val_loss: 0.00019\n",
      "epoch: 45 | train_loss: 0.00015 | val_loss: 0.00016\n",
      "epoch: 46 | train_loss: 0.00013 | val_loss: 0.00014\n",
      "epoch: 47 | train_loss: 0.00011 | val_loss: 0.00012\n",
      "epoch: 48 | train_loss: 0.00010 | val_loss: 0.00011\n",
      "epoch: 49 | train_loss: 0.00009 | val_loss: 0.00009\n",
      "epoch: 50 | train_loss: 0.00008 | val_loss: 0.00008\n",
      "epoch: 51 | train_loss: 0.00008 | val_loss: 0.00008\n",
      "epoch: 52 | train_loss: 0.00007 | val_loss: 0.00007\n",
      "epoch: 53 | train_loss: 0.00006 | val_loss: 0.00006\n",
      "epoch: 54 | train_loss: 0.00006 | val_loss: 0.00006\n",
      "epoch: 55 | train_loss: 0.00006 | val_loss: 0.00006\n",
      "epoch: 56 | train_loss: 0.00005 | val_loss: 0.00005\n",
      "epoch: 57 | train_loss: 0.00005 | val_loss: 0.00005\n",
      "epoch: 58 | train_loss: 0.00005 | val_loss: 0.00005\n",
      "epoch: 59 | train_loss: 0.00005 | val_loss: 0.00005\n",
      "epoch: 60 | train_loss: 0.00005 | val_loss: 0.00005\n",
      "epoch: 61 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 62 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 63 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 64 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 65 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 66 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 67 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 68 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 69 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 70 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 71 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 72 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 73 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 74 | train_loss: 0.00004 | val_loss: 0.00005\n",
      "epoch: 75 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 76 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 77 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 78 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 79 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 80 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 81 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 82 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 83 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 84 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 85 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 86 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 87 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 88 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 89 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 90 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 91 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 92 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 93 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 94 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 95 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 96 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 97 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 98 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 99 | train_loss: 0.00003 | val_loss: 0.00005\n",
      "epoch: 100 | train_loss: 0.00003 | val_loss: 0.00005\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # train\n",
    "    i, train_loss, val_loss = 0, 0., 0.\n",
    "    for x, y in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model.init_states(batch_size=x.shape[0])\n",
    "        out = model(x)\n",
    "\n",
    "        loss = criterion(out, y.float().view(-1, 1))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "    train_loss /= i\n",
    "\n",
    "    # validate\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for x, y in val_data_loader:\n",
    "            model.init_states(batch_size=x.shape[0])\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y.float().view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "            i += 1\n",
    "        val_loss /= i\n",
    "\n",
    "    print('epoch: {} | train_loss: {:0.5f} | val_loss: {:0.5f}'.format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb11ef39e40>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test\n",
    "with torch.no_grad():\n",
    "    for x, y in val_data_loader:\n",
    "        model.init_states(batch_size=x.shape[0])\n",
    "        out = model(x)\n",
    "        break\n",
    "        #loss = criterion(out, y.float().view(-1, 1))\n",
    "        \n",
    "# Plot\n",
    "plt.plot(y.float(), 'o', label='y')\n",
    "plt.plot(out.view(-1), '^', label='yhat')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

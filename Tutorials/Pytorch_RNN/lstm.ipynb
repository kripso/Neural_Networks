{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# generate a dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, n_grads, H):\n",
    "        super().__init__()\n",
    "        self.grads = np.random.uniform(-1, 1, size=n_grads)\n",
    "        self.x = torch.arange(0, 1, 0.01)\n",
    "        self.H = H # horizon [steps]\n",
    "        self.interval = H//2 # [steps]\n",
    "\n",
    "        self.len = n_grads\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        1. sample `grads`\n",
    "        2. generate linear data (seq_len*input_size)\n",
    "        Remarks: (batch_size*seq_len*input_size) is formed by DataLoader.\n",
    "        \"\"\"\n",
    "        H = self.H\n",
    "\n",
    "        # 1.\n",
    "        grad = self.grads[idx]\n",
    "\n",
    "        # 2.\n",
    "        y = grad * self.x\n",
    "\n",
    "        subys = torch.tensor([])  # (seq_len*input_size(H))\n",
    "        i = 0\n",
    "        while len(y[i:i+H]) == H:\n",
    "            suby = y[i:i+H].view(1, -1)\n",
    "            subys = torch.cat((subys, suby), 0)\n",
    "            i += self.interval\n",
    "\n",
    "        return subys, grad\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.h_size = 32\n",
    "        self.n_layers = 2\n",
    "\n",
    "        # define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.in_size, self.h_size, self.n_layers, \n",
    "                            batch_first=True)\n",
    "        self.h_n = None  # hidden state\n",
    "        self.c_n = None  # cell state\n",
    "\n",
    "        # define the output layer\n",
    "        self.linear = nn.Linear(self.h_size, 1)  # predicts `linear-grad`\n",
    "\n",
    "    def init_states(self, batch_size):\n",
    "        \"\"\"initialize the hidden and cell states\"\"\"\n",
    "        self.h_n = torch.zeros(self.n_layers, batch_size, self.h_size)\n",
    "        self.c_n = torch.zeros(self.n_layers, batch_size, self.h_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_size)\n",
    "\n",
    "        out: (batch, seq_len, hidden_size)\n",
    "        h_n, c_n: (num_layers * num_directions, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        states = (self.h_n, self.c_n)\n",
    "        out, (self.h_n, self.c_n) = self.lstm(x, states)\n",
    "\n",
    "        # get the last one of the (sequential) output from the LSTM\n",
    "        out = out[:, -1, :]  # (batch, hidden_size)\n",
    "        out = self.linear(out)  # (batch, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_dataset = MyDataset(n_grads=30, H=10)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=8)\n",
    "\n",
    "val_dataset = MyDataset(n_grads=10, H=10)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "\n",
    "# Model\n",
    "model = LSTM(in_size=train_dataset.H)\n",
    "\n",
    "\n",
    "# Compile\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | train_loss: 0.39635 | val_loss: 0.44035\n",
      "epoch: 2 | train_loss: 0.35664 | val_loss: 0.40485\n",
      "epoch: 3 | train_loss: 0.32004 | val_loss: 0.36949\n",
      "epoch: 4 | train_loss: 0.28282 | val_loss: 0.33024\n",
      "epoch: 5 | train_loss: 0.24198 | val_loss: 0.28340\n",
      "epoch: 6 | train_loss: 0.19625 | val_loss: 0.22813\n",
      "epoch: 7 | train_loss: 0.14959 | val_loss: 0.17093\n",
      "epoch: 8 | train_loss: 0.11338 | val_loss: 0.12470\n",
      "epoch: 9 | train_loss: 0.09240 | val_loss: 0.09021\n",
      "epoch: 10 | train_loss: 0.06962 | val_loss: 0.05967\n",
      "epoch: 11 | train_loss: 0.04540 | val_loss: 0.03935\n",
      "epoch: 12 | train_loss: 0.03159 | val_loss: 0.02717\n",
      "epoch: 13 | train_loss: 0.02373 | val_loss: 0.01700\n",
      "epoch: 14 | train_loss: 0.01802 | val_loss: 0.01161\n",
      "epoch: 15 | train_loss: 0.01618 | val_loss: 0.01098\n",
      "epoch: 16 | train_loss: 0.01531 | val_loss: 0.01039\n",
      "epoch: 17 | train_loss: 0.01314 | val_loss: 0.00950\n",
      "epoch: 18 | train_loss: 0.01067 | val_loss: 0.00817\n",
      "epoch: 19 | train_loss: 0.00821 | val_loss: 0.00627\n",
      "epoch: 20 | train_loss: 0.00630 | val_loss: 0.00471\n",
      "epoch: 21 | train_loss: 0.00520 | val_loss: 0.00379\n",
      "epoch: 22 | train_loss: 0.00439 | val_loss: 0.00332\n",
      "epoch: 23 | train_loss: 0.00365 | val_loss: 0.00314\n",
      "epoch: 24 | train_loss: 0.00303 | val_loss: 0.00294\n",
      "epoch: 25 | train_loss: 0.00253 | val_loss: 0.00261\n",
      "epoch: 26 | train_loss: 0.00216 | val_loss: 0.00221\n",
      "epoch: 27 | train_loss: 0.00186 | val_loss: 0.00180\n",
      "epoch: 28 | train_loss: 0.00156 | val_loss: 0.00145\n",
      "epoch: 29 | train_loss: 0.00130 | val_loss: 0.00116\n",
      "epoch: 30 | train_loss: 0.00108 | val_loss: 0.00091\n",
      "epoch: 31 | train_loss: 0.00091 | val_loss: 0.00074\n",
      "epoch: 32 | train_loss: 0.00076 | val_loss: 0.00064\n",
      "epoch: 33 | train_loss: 0.00064 | val_loss: 0.00056\n",
      "epoch: 34 | train_loss: 0.00054 | val_loss: 0.00047\n",
      "epoch: 35 | train_loss: 0.00045 | val_loss: 0.00037\n",
      "epoch: 36 | train_loss: 0.00038 | val_loss: 0.00029\n",
      "epoch: 37 | train_loss: 0.00033 | val_loss: 0.00023\n",
      "epoch: 38 | train_loss: 0.00028 | val_loss: 0.00019\n",
      "epoch: 39 | train_loss: 0.00024 | val_loss: 0.00016\n",
      "epoch: 40 | train_loss: 0.00020 | val_loss: 0.00013\n",
      "epoch: 41 | train_loss: 0.00018 | val_loss: 0.00011\n",
      "epoch: 42 | train_loss: 0.00016 | val_loss: 0.00009\n",
      "epoch: 43 | train_loss: 0.00014 | val_loss: 0.00008\n",
      "epoch: 44 | train_loss: 0.00013 | val_loss: 0.00007\n",
      "epoch: 45 | train_loss: 0.00012 | val_loss: 0.00006\n",
      "epoch: 46 | train_loss: 0.00011 | val_loss: 0.00006\n",
      "epoch: 47 | train_loss: 0.00010 | val_loss: 0.00005\n",
      "epoch: 48 | train_loss: 0.00009 | val_loss: 0.00005\n",
      "epoch: 49 | train_loss: 0.00009 | val_loss: 0.00005\n",
      "epoch: 50 | train_loss: 0.00009 | val_loss: 0.00005\n",
      "epoch: 51 | train_loss: 0.00008 | val_loss: 0.00005\n",
      "epoch: 52 | train_loss: 0.00008 | val_loss: 0.00005\n",
      "epoch: 53 | train_loss: 0.00008 | val_loss: 0.00005\n",
      "epoch: 54 | train_loss: 0.00008 | val_loss: 0.00005\n",
      "epoch: 55 | train_loss: 0.00008 | val_loss: 0.00005\n",
      "epoch: 56 | train_loss: 0.00007 | val_loss: 0.00005\n",
      "epoch: 57 | train_loss: 0.00007 | val_loss: 0.00005\n",
      "epoch: 58 | train_loss: 0.00007 | val_loss: 0.00005\n",
      "epoch: 59 | train_loss: 0.00007 | val_loss: 0.00005\n",
      "epoch: 60 | train_loss: 0.00007 | val_loss: 0.00005\n",
      "epoch: 61 | train_loss: 0.00007 | val_loss: 0.00005\n",
      "epoch: 62 | train_loss: 0.00007 | val_loss: 0.00005\n",
      "epoch: 63 | train_loss: 0.00007 | val_loss: 0.00004\n",
      "epoch: 64 | train_loss: 0.00007 | val_loss: 0.00004\n",
      "epoch: 65 | train_loss: 0.00007 | val_loss: 0.00004\n",
      "epoch: 66 | train_loss: 0.00007 | val_loss: 0.00004\n",
      "epoch: 67 | train_loss: 0.00007 | val_loss: 0.00004\n",
      "epoch: 68 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 69 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 70 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 71 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 72 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 73 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 74 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 75 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 76 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 77 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 78 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 79 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 80 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 81 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 82 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 83 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 84 | train_loss: 0.00006 | val_loss: 0.00004\n",
      "epoch: 85 | train_loss: 0.00005 | val_loss: 0.00004\n",
      "epoch: 86 | train_loss: 0.00005 | val_loss: 0.00004\n",
      "epoch: 87 | train_loss: 0.00005 | val_loss: 0.00003\n",
      "epoch: 88 | train_loss: 0.00005 | val_loss: 0.00003\n",
      "epoch: 89 | train_loss: 0.00005 | val_loss: 0.00003\n",
      "epoch: 90 | train_loss: 0.00005 | val_loss: 0.00003\n",
      "epoch: 91 | train_loss: 0.00005 | val_loss: 0.00003\n",
      "epoch: 92 | train_loss: 0.00005 | val_loss: 0.00003\n",
      "epoch: 93 | train_loss: 0.00005 | val_loss: 0.00003\n",
      "epoch: 94 | train_loss: 0.00005 | val_loss: 0.00003\n",
      "epoch: 95 | train_loss: 0.00005 | val_loss: 0.00003\n",
      "epoch: 96 | train_loss: 0.00005 | val_loss: 0.00003\n",
      "epoch: 97 | train_loss: 0.00005 | val_loss: 0.00003\n",
      "epoch: 98 | train_loss: 0.00005 | val_loss: 0.00003\n",
      "epoch: 99 | train_loss: 0.00005 | val_loss: 0.00003\n",
      "epoch: 100 | train_loss: 0.00005 | val_loss: 0.00003\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # train\n",
    "    i, train_loss, val_loss = 0, 0., 0.\n",
    "    for x, y in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model.init_states(batch_size=x.shape[0])\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y.float().view(-1, 1))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "    train_loss /= i\n",
    "\n",
    "    # validate\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for x, y in val_data_loader:\n",
    "            model.init_states(batch_size=x.shape[0])\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y.float().view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "            i += 1\n",
    "        val_loss /= i\n",
    "\n",
    "    print('epoch: {} | train_loss: {:0.5f} | val_loss: {:0.5f}'.format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc654aac5e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test\n",
    "with torch.no_grad():\n",
    "    for x, y in val_data_loader:\n",
    "        model.init_states(batch_size=x.shape[0])\n",
    "        out = model(x)\n",
    "        break\n",
    "        #loss = criterion(out, y.float().view(-1, 1))\n",
    "        \n",
    "# Plot\n",
    "plt.plot(y.float(), 'o', label='y')\n",
    "plt.plot(out.view(-1), '^', label='yhat')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
